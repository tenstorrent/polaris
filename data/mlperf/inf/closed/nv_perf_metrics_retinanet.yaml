- benchmark: Benchmark.Retinanet
  cores: 56
  gpu: NVIDIA H200-SXM-141GB
  gpu_batch_size:
    retinanet: 48
  gpu_copy_streams: 2
  gpu_inference_streams: 2
  id: 4.1-0048
  input_dtype: int8
  input_format: linear
  map_path: data_maps/open-images-v6-mlperf/val_map.txt
  metric: Samples/s
  nodes: 1
  num gpu: 8
  offline_expected_qps: 13600
  org: NVIDIA
  perf: 14439.0
  precision: int8
  processor: Intel(R) Xeon(R) Platinum 8480C
  run_infer_on_copy_streams: false
  scenario: Scenario.Offline
  sys: NVIDIA H200 (8x H200-SXM-141GB TensorRT)
  system: KnownSystem.H200_SXM_141GBx8
  use_graphs: false
  wlname: retinanet
  workspace_size: 128000000000
- active_sms: 100
  benchmark: Benchmark.Retinanet
  cores: 56
  deque_timeout_usec: 30000
  gpu: NVIDIA H200-SXM-141GB
  gpu_batch_size:
    retinanet: 8
  gpu_copy_streams: 4
  gpu_inference_streams: 2
  id: 4.1-0048
  input_dtype: int8
  input_format: linear
  map_path: data_maps/open-images-v6-mlperf/val_map.txt
  metric: Queries/s
  nodes: 1
  num gpu: 8
  org: NVIDIA
  perf: 13604.0
  precision: int8
  processor: Intel(R) Xeon(R) Platinum 8480C
  scenario: Scenario.Server
  server_target_qps: 13600
  start_from_device: true
  sys: NVIDIA H200 (8x H200-SXM-141GB TensorRT)
  system: KnownSystem.H200_SXM_141GBx8
  use_cuda_thread_per_device: true
  use_deque_limit: true
  use_graphs: false
  wlname: retinanet
  workspace_size: 60000000000

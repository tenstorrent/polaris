_cfgName,accelerator_frequency,accelerator_host_interconnect,accelerator_interconnect,accelerator_interconnect_topology,accelerator_memory_capacity,accelerator_memory_configuration,accelerator_model_name,accelerator_on-chip_memories,accelerators_per_node,boot_firmware_version,cooling,disk_controllers,disk_drives,division,filesystem,framework,host_memory_capacity,host_memory_configuration,host_network_card_count,host_networking,host_networking_topology,host_processor_caches,host_processor_core_count,host_processor_frequency,host_processor_interconnect,host_processor_model_name,host_processors_per_node,host_storage_capacity,host_storage_type,hw_notes,management_firmware_version,network_speed_mbit,nics_enabled_connected,nics_enabled_firmware,nics_enabled_os,number_of_nodes,number_of_type_nics_installed,operating_system,other_hardware,other_software_stack,power_management,power_supply_details,power_supply_quantity_and_rating_watts,status,submitter,sw_notes,system_name,system_type,system_type_detail
H200-SXM-141GB-CTSx8_TRT,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3e,NVIDIA H200-SXM-141GB-CTS,,8,,Air/liquid cooling,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA12,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 1000W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA H200 (8x H200-SXM-141GB-CTS, TensorRT)",datacenter,N/A
H200-SXM-141GBx8_TRT_MaxQ,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3e,NVIDIA H200-SXM-141GB,,8,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA1,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 700W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA H200 (8x H200-SXM-141GB, MaxQ, TensorRT)",datacenter,N/A
H200-SXM-141GBx8_TRT,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3e,NVIDIA H200-SXM-141GB,,8,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA1,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 700W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",datacenter,N/A
GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT,,NVLink-C2C,1x 400Gbe Infiniband,,144 GB,HBM3e,NVIDIA GH200 Grace Hopper Superchip 144GB,,1,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",480 GB,15x 16DP (32GB) LPDDR5x,1x Mellanox MT2894 [ConnectX-6 Lx],Ethernet(IPoIB); Data bandwidth for GPU-NIC is 252.06 GB/s,Ethernet(IPoIB)/Infiniband on switching network,,72,,,NVIDIA Grace CPU,1,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",NVIDIA GH200 144GB HBM3e,,,,,,1,,Ubuntu 22.04.2,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 560.30",,,,available,NVIDIA,,"NVIDIA GH200 NVL2 Platform (1x GH200-144GB_aarch64, TensorRT)",datacenter,N/A
DGX-H100_H100-SXM-80GBx8_TRT,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,80 GB,HBM3,NVIDIA H100-SXM-80GB,,8,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA1,10x 400Gbe Infiniband,Infiniband; Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT)",datacenter,N/A
H200-SXM-141GBx1_TRT,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3,NVIDIA H200-SXM-141GB,,1,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA1,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 700W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA H200 (1x H200-SXM-141GB, TensorRT)",datacenter,N/A
B200-SXM-180GBx1_TRT,,PCIe Gen5 x16,N/A,,180 GB,HBM3e,NVIDIA B200-SXM-180GB,,1,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.1.0, CUDA 12.7",408GB,6x 64GB MTC40F2046S1RC48BA1,2x 10Gbe,Gig Ethernet,Ethernet on switching network,,12,,,Intel(R) Xeon(R) Silver 4410Y,1,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",B200 TGP 1000W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.1.0, CUDA 12.7, cuDNN 8.9.7, Driver 565",,,,preview,NVIDIA,Private git hash for code and TRTLLM were used to generate the preview results. The git hash are eba031f9e2e6cf3d1cdce0549511d27adf01a3f4 and 4e5b175cc80320789ba6846ced80a87f25e70fb2,"NVIDIA B200 (1x B200-SXM-180GB, TensorRT)",datacenter,N/A
H200-SXM-141GB-CTSx1_TRT,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3e,NVIDIA H200-SXM-141GB-CTS,,1,,Air/liquid cooling,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA12,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 1000W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54",,,,available,NVIDIA,,"NVIDIA H200 (1x H200-SXM-141GB-CTS, TensorRT)",datacenter,N/A
H200-SXM-141GBx8_TRT_Triton,,PCIe Gen5 x16,"18x 4th Gen NVLink, 900GB/s",,141 GB,HBM3e,NVIDIA H200-SXM-141GB,,8,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",2 TB,32x 64GB MTC40F2046S1RC48BA1,10x 400Gbe Infiniband,Infiniband;  Data bandwidth for GPU-PCIe: 504GB/s; PCIe-NIC: 500GB/s,Ethernet/Infiniband on switching network,,56,,,Intel(R) Xeon(R) Platinum 8480C,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",H200 TGP 700W,,,,,,1,,Ubuntu 22.04.4,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 550.54, Triton 24.06",,,,available,NVIDIA,,"NVIDIA H200 (8x H200-SXM-141GB, TensorRT, Triton)",datacenter,N/A
Orin_TRT,,N/A,N/A,,Shared with host,LPDDR5,NVIDIA Jetson AGX Orin 64G,,1,,Air-cooled,eMMC 5.1,eMMC 5.1,closed,,"Jetpack 6.0, TensorRT 10.1, CUDA 12.2",64 GB,64GB 256-bit LPDDR5,1 Integrated,Gig Ethernet,USB forwarded,,12,,,12-core ARM Cortex-A78AE CPU,1,"64 GB eMMC, 5TB CIFS","eMMC 5.1, CIFS mounted disk storage","GPU and both DLAs are used in resnet50 and Retinanet, in Offline scenario",,,,,,1,,Jetson r36.3.1 L4T,,"Jetpack 6.0, TensorRT 10.1, CUDA 12.2, cuDNN 8.9.4",,Dell USB-C 130.0W Adapter (HA130PM170),130W,available,NVIDIA,,NVIDIA Jetson AGX Orin Developer Kit 64G (TensorRT),edge,N/A
GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT,,NVLink-C2C,1x 400Gbe Infiniband,,144 GB,HBM3e,NVIDIA GH200 Grace Hopper Superchip 144GB,,2,,Air-cooled,NVMe,SSD,closed,,"TensorRT 10.2.0, CUDA 12.4",960 GB,15x 16DP (32GB) LPDDR5x,1x Mellanox MT2894 [ConnectX-6 Lx],Ethernet(IPoIB); Data bandwidth for GPU-NIC is 252.06 GB/s,Ethernet(IPoIB)/Infiniband on switching network,,144,,,NVIDIA Grace CPU,2,"2 TB SSD, 5 TB CIFS","NVMe SSD, CIFS mounted disk storage",NVIDIA GH200 144GB HBM3e,,,,,,1,,Ubuntu 22.04.2,,"TensorRT 10.2.0, CUDA 12.4, cuDNN 8.9.7, Driver 560.30",,,,available,NVIDIA,,"NVIDIA GH200 NVL2 Platform (2x GH200-144GB_aarch64, TensorRT)",datacenter,N/A
